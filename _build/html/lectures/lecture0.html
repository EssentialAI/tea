
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Linear Algebra Review &#8212; The Essential AI</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe,.cell"
        const thebe_selector_input = "pre,.cell_input div.highlight"
        const thebe_selector_output = ".output,.cell_output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="shortcut icon" href="../_static/brain1.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Lecture 1: Introduction to Machine Learning" href="Lecture-1.html" />
    <link rel="prev" title="The Essential AI" href="../index.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/lightmode.svg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">The Essential AI</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   The Essential AI
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Linear Algebra Review
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Lecture-1.html">
   Lecture 1: Introduction to Machine Learning
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/lectures/lecture0.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/EssentialAI/cs229/main?urlpath=tree/lectures/lecture0.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <button type="button" class="btn btn-secondary topbarbtn"
            onclick="initThebeSBT()" title="Launch Thebe" data-toggle="tooltip" data-placement="left"><i
                class="fas fa-play"></i><span style="margin-left: .4em;">Live Code</span></button>
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#matrix-multiplication">
   1. Matrix Multiplication
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#vector-vector-multiplication">
     1.1. Vector-Vector Multiplication
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#matrix-vector-products">
     1.2. Matrix-Vector Products
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#matrix-matrix-products">
     1.3. Matrix-Matrix Products
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#operations-and-properties">
   2. Operations and Properties
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#symmetric-matrices">
     2.1. Symmetric Matrices
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#norms">
     2.2. Norms
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-independence-and-rank">
     2.3. Linear Independence and Rank
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-inverse-of-a-square-matrix">
     2.4. The Inverse of a Square Matrix
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#orthogonal-matrices">
     2.5. Orthogonal Matrices
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#range-and-nullspace-of-a-matrix">
     2.6. Range and nullspace of a Matrix
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#quadratic-forms-and-positive-semidefinite-matrices">
     2.7. Quadratic Forms and Positive Semidefinite Matrices
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#eigenvalues-and-eigenvectors">
     2.8. Eigenvalues and Eigenvectors
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#eigenvalues-and-eigenvectors-of-symmetric-matrices">
     2.9. Eigenvalues and Eigenvectors of Symmetric Matrices
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#diagonalizing-matrix-vector-multiplication">
     2.10. Diagonalizing matrix-vector multiplication:
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Linear Algebra Review</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#matrix-multiplication">
   1. Matrix Multiplication
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#vector-vector-multiplication">
     1.1. Vector-Vector Multiplication
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#matrix-vector-products">
     1.2. Matrix-Vector Products
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#matrix-matrix-products">
     1.3. Matrix-Matrix Products
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#operations-and-properties">
   2. Operations and Properties
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#symmetric-matrices">
     2.1. Symmetric Matrices
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#norms">
     2.2. Norms
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-independence-and-rank">
     2.3. Linear Independence and Rank
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-inverse-of-a-square-matrix">
     2.4. The Inverse of a Square Matrix
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#orthogonal-matrices">
     2.5. Orthogonal Matrices
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#range-and-nullspace-of-a-matrix">
     2.6. Range and nullspace of a Matrix
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#quadratic-forms-and-positive-semidefinite-matrices">
     2.7. Quadratic Forms and Positive Semidefinite Matrices
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#eigenvalues-and-eigenvectors">
     2.8. Eigenvalues and Eigenvectors
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#eigenvalues-and-eigenvectors-of-symmetric-matrices">
     2.9. Eigenvalues and Eigenvectors of Symmetric Matrices
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#diagonalizing-matrix-vector-multiplication">
     2.10. Diagonalizing matrix-vector multiplication:
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="linear-algebra-review">
<h1>Linear Algebra Review<a class="headerlink" href="#linear-algebra-review" title="Permalink to this headline">¶</a></h1>
<p>Linear algebra provides a way of compactly representing and operating on sets of linear equations. For example:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
4x_{1}-5x_{2} &amp;= -13 \\
-2x_{1}+3x_{2} &amp;= 9
\end{align}
\end{split}\]</div>
<p style="text-align:center">The matrix notation of above equations is:</p>
<div class="math notranslate nohighlight">
\[Ax =b\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\text{with } A = \begin{bmatrix}
      4 &amp; -5 \\
      -2 &amp; 3
      \end{bmatrix}, \enspace b = \begin{bmatrix}
      -13 \\
      9
      \end{bmatrix}\end{split}\]</div>
<br>      
<p>By <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{m \times n}\)</span>, we denote a matrix with <span class="math notranslate nohighlight">\(m\)</span> rows and <span class="math notranslate nohighlight">\(n\)</span> columns. By <span class="math notranslate nohighlight">\(x \in \mathbb{R}^n\)</span>, we denote vector with <span class="math notranslate nohighlight">\(n\)</span> entries.</p>
<br>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}A = \begin{bmatrix}
      a_{11} &amp; a_{12} &amp; a_{13} &amp; ... &amp; a_{1n} \\
      a_{21} &amp; a_{22} &amp; a_{23} &amp; ... &amp; a_{2n} \\
      a_{31} &amp; a_{32} &amp; a_{33} &amp; ... &amp; a_{3n} \\
      ... &amp; ... &amp; ... &amp; ... &amp; ... \\
      a_{m1} &amp; a_{m2} &amp; a_{m3} &amp; ... &amp; a_{mn}
      \end{bmatrix}, \enspace \enspace x = \begin{bmatrix}
      x_{1} \\
      x_{2} \\
      x_{3} \\
      .. \\
      x_{n}
      \end{bmatrix}
      \end{align}\end{split}\]</div>
<br>
<p>We denote the <span class="math notranslate nohighlight">\(j\)</span>th column of <span class="math notranslate nohighlight">\(A\)</span> by <span class="math notranslate nohighlight">\(a^j\)</span> or <span class="math notranslate nohighlight">\(A_{:,j}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}A = \begin{bmatrix}
      | &amp; | &amp; | &amp; ... &amp; | \\
      a^1 &amp; a^2 &amp; a^3 &amp; ... &amp; a^n \\
      | &amp; | &amp; | &amp; ... &amp;|
      \end{bmatrix}\end{split}\]</div>
<br>
<p>We denote the <span class="math notranslate nohighlight">\(i\)</span>th row of <span class="math notranslate nohighlight">\(A\)</span> by <span class="math notranslate nohighlight">\(a^T\)</span> or <span class="math notranslate nohighlight">\(A_{i,:}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}A = \begin{bmatrix}
      - a_{1}^T - \\
      - a_{2}^T - \\
      .. \\
      - a_{m}^T - 
      \end{bmatrix}\end{split}\]</div>
<br>      
<div class="section" id="matrix-multiplication">
<h2>1. Matrix Multiplication<a class="headerlink" href="#matrix-multiplication" title="Permalink to this headline">¶</a></h2>
<p>The product of two matrices <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{m \times n}\)</span> and <span class="math notranslate nohighlight">\(B \in \mathbb{R}^{n \times p}\)</span> is the matrix</p>
<div class="math notranslate nohighlight">
\[
C = AB \in \mathbb{R}^{m \times p} \enspace \text{where, } C_{ij} = \sum_{k=1}^{n}A_{ik}B_{kj}\]</div>
<p>Note that in order for the matrix product to exist, the number of columns in <span class="math notranslate nohighlight">\(A\)</span> must equal the number of rows in <span class="math notranslate nohighlight">\(B\)</span>.</p>
<div class="section" id="vector-vector-multiplication">
<h3>1.1. Vector-Vector Multiplication<a class="headerlink" href="#vector-vector-multiplication" title="Permalink to this headline">¶</a></h3>
<p>Given two vectors <span class="math notranslate nohighlight">\(x,y \in \mathbb{R}^n\)</span>, the quantity <span class="math notranslate nohighlight">\(x^Ty\)</span>, sometimes called the <span class = 'high'>inner product or dot product</span> of the vectors, is a real number given by</p>
<br>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align} x^Ty \in \mathbb{R} = \begin{matrix} [x_{1} &amp; x_{2} &amp; ... &amp; x_{n}]\end{matrix} \begin{bmatrix}
      y_{1} \\
      y_{2} \\
      .. \\
      y_{n}
      \end{bmatrix} = \sum_{i=1}^{n}x_{i}y_{i}\end{align}\end{split}\]</div>
<br>
<p>Given vectors <span class="math notranslate nohighlight">\(x \in \mathbb{R}^m, y \in \mathbb{R}^n\)</span> (not necessarily of the same size), <span class="math notranslate nohighlight">\(xy^T \in \mathbb{R}^{m \times n}\)</span> is called the <span class = 'high'>outer product</span> of the vectors. It is a matrix, whose entries are given by <span class="math notranslate nohighlight">\((xy^T)_{ij} = x_iy_j\)</span>:</p>
<br>
<div class="math notranslate nohighlight">
\[\begin{split}xy^T \in \mathbb{R}^{m \times n} = \begin{bmatrix}
      x_{1} \\
      x_{2} \\
      .. \\
      x_{n}
      \end{bmatrix}\begin{matrix} [y_{1} &amp; y_{2} &amp; ... &amp; y_{n}]\end{matrix} = \begin{bmatrix}
      x_{1}y_{1} &amp; x_{1}y_{2} &amp; ... &amp; x_{1}y_{n} \\
      x_{2}y_{1} &amp; x_{2}y_{2} &amp; ... &amp; x_{2}y_{n} \\
      ... &amp; ... &amp; ... &amp; ... \\
      x_{m}y_{1} &amp; x_{m}y_{2} &amp; ... &amp; x_{m}y_{n} \\
      \end{bmatrix}\end{split}\]</div>
<br>
</div>
<div class="section" id="matrix-vector-products">
<h3>1.2. Matrix-Vector Products<a class="headerlink" href="#matrix-vector-products" title="Permalink to this headline">¶</a></h3>
<p>Given a matrix <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{m \times n}\)</span> and a vector <span class="math notranslate nohighlight">\(x \in \mathbb{R}^n\)</span>, their product is a vector <span class="math notranslate nohighlight">\(y = Ax \in \mathbb{R}^m\)</span>. There are a couple of ways of looking at matrix-vector multiplication, and we will look at each of them in turn.</p>
<p>If we write <span class="math notranslate nohighlight">\(A\)</span> by rows, then we can express <span class="math notranslate nohighlight">\(Ax\)</span> as,</p>
<br>
<div class="math notranslate nohighlight">
\[\begin{split}y = Ax = \begin{bmatrix}
      - a_{1}^T - \\
      - a_{2}^T - \\
      .. \\
      - a_{m}^T - 
      \end{bmatrix}x = \begin{bmatrix}
      a_{1}^Tx\\
      a_{2}^Tx \\
      .. \\
      a_{m}^Tx 
      \end{bmatrix}\end{split}\]</div>
<br>
<p>In other words, the <span class="math notranslate nohighlight">\(i\)</span>th entry of <span class="math notranslate nohighlight">\(y\)</span> is equal to the inner product of the <span class="math notranslate nohighlight">\(i\)</span>th row of <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(x\)</span>, <span class="math notranslate nohighlight">\(y_i=a_i^Tx\)</span>.</p>
<p>Alternatively, let’s write <span class="math notranslate nohighlight">\(A\)</span> in column form. In this case we see that,</p>
<br>
<div class="math notranslate nohighlight">
\[\begin{split} y = Ax = \begin{bmatrix}
      | &amp; | &amp; | &amp; ... &amp; | \\
      a^1 &amp; a^2 &amp; a^3 &amp; ... &amp; a^n \\
      | &amp; | &amp; | &amp; ... &amp;|
      \end{bmatrix} \begin{bmatrix}
      x_{1} \\
      x_{2} \\
      x_{3} \\
      .. \\
      x_{n}
      \end{bmatrix} = \begin{matrix}
      a^1
      \end{matrix}x_1+\begin{matrix}
      a^2
      \end{matrix}x_2+...+\begin{matrix}
      a^n
      \end{matrix}x_n \color{blue}{\enspace \rightarrow \enspace (1)}
\end{split}\]</div>
<div class="math notranslate nohighlight" id="equation-1">
<span class="eqno">(1)<a class="headerlink" href="#equation-1" title="Permalink to this equation">¶</a></span>\[\begin{split} y = Ax = \begin{bmatrix}
      | &amp; | &amp; | &amp; ... &amp; | \\
      a^1 &amp; a^2 &amp; a^3 &amp; ... &amp; a^n \\
      | &amp; | &amp; | &amp; ... &amp;|
      \end{bmatrix} \begin{bmatrix}
      x_{1} \\
      x_{2} \\
      x_{3} \\
      .. \\
      x_{n}
      \end{bmatrix} = \begin{matrix}
      a^1
      \end{matrix}x_1+\begin{matrix}
      a^2
      \end{matrix}x_2+...+\begin{matrix}
      a^n
      \end{matrix}x_n
\end{split}\]</div>
<br>
<p>In other words, <span class="math notranslate nohighlight">\(y\)</span> is a <strong>linear combination</strong> of the columns of <span class="math notranslate nohighlight">\(A\)</span>, where the coefficients of the linear combination are given by the entries of <span class="math notranslate nohighlight">\(x\)</span>.</p>
</div>
<div class="section" id="matrix-matrix-products">
<h3>1.3. Matrix-Matrix Products<a class="headerlink" href="#matrix-matrix-products" title="Permalink to this headline">¶</a></h3>
<p>Using the above information, we can view matrix-matrix multiplication from various perspectives. One of them is to view the matrix-matrix multiplication as a set of vector-vector products. The most obvious viewpoint, which follows immediately from the definition, is that the <span class="math notranslate nohighlight">\((i,j)\)</span>th entry of <span class="math notranslate nohighlight">\(C\)</span> is equal to the inner product of the <span class="math notranslate nohighlight">\(i\)</span>th row of <span class="math notranslate nohighlight">\(A\)</span> and the <span class="math notranslate nohighlight">\(j\)</span>th column of <span class="math notranslate nohighlight">\(B\)</span>. Symbolically, this looks like the following:</p>
<br>
<div class="math notranslate nohighlight">
\[\begin{split}C = AB = \begin{bmatrix}
      - a_{1}^T - \\
      - a_{2}^T - \\
      .. \\
      - a_{m}^T - 
      \end{bmatrix} \begin{bmatrix}
      | &amp; | &amp; | &amp; ... &amp; | \\
      b^1 &amp; b^2 &amp; b^3 &amp; ... &amp; b^n \\
      | &amp; | &amp; | &amp; ... &amp;|
      \end{bmatrix}=\begin{bmatrix}
      a_1^Tb^1 &amp; a_1^Tb^2 &amp; ... &amp; a_1^Tb^p \\
      a_2^Tb^1 &amp; a_2^Tb^2 &amp; ... &amp; a_2^Tb^p \\
      .. &amp; .. &amp; ... &amp; .. \\
      a_m^Tb^1 &amp; a_m^Tb^2 &amp; ... &amp; a_m^Tb^p
      \end{bmatrix} \end{split}\]</div>
<br>
<p>Remember that since <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{m \times n}\)</span> and <span class="math notranslate nohighlight">\(B \in \mathbb{R}^{n \times p}, a_i \in \mathbb{R}^n \text{ and } b^j \in \mathbb{R}^n\)</span>, so these inner products all make sense. This is the ‘natural’ representation when we represent <span class="math notranslate nohighlight">\(A\)</span> by rows and <span class="math notranslate nohighlight">\(B\)</span> by columns. Alternatively, we can represent <span class="math notranslate nohighlight">\(A\)</span> by columns, and <span class="math notranslate nohighlight">\(B\)</span> by rows. This representation leads to a much tricker interpretation of <span class="math notranslate nohighlight">\(AB\)</span> as a sum of outer products. Symbolically,</p>
<br>
<div class="math notranslate nohighlight">
\[\begin{split} C = AB = \begin{bmatrix}
      | &amp; | &amp; | &amp; ... &amp; | \\
      a^1 &amp; a^2 &amp; a^3 &amp; ... &amp; a^n \\
      | &amp; | &amp; | &amp; ... &amp;|
      \end{bmatrix} \begin{bmatrix}
      - b_{1}^T - \\
      - b_{2}^T - \\
      .. \\
      - b_{n}^T - 
      \end{bmatrix} = \sum_{i=1}^{n}a^ib_{i}^T \end{split}\]</div>
<br>
<p>Put another way, <span class="math notranslate nohighlight">\(AB\)</span> is equal to the sum, over all <span class="math notranslate nohighlight">\(i\)</span>, of the outer product of the <span class="math notranslate nohighlight">\(i\)</span>th column of <span class="math notranslate nohighlight">\(A\)</span> and the <span class="math notranslate nohighlight">\(i\)</span>th row of <span class="math notranslate nohighlight">\(B\)</span>. Since, in this case, <span class="math notranslate nohighlight">\(a^i \in \mathbb{R}^m\)</span> and <span class="math notranslate nohighlight">\(b_i \in \mathbb{R}^p\)</span>, the dimension of the outer product <span class="math notranslate nohighlight">\(a^ib_i^T\)</span> is <span class="math notranslate nohighlight">\(m \times p\)</span>, which coincides with the dimension of <span class="math notranslate nohighlight">\(C\)</span>. Chances are, the last equality above may appear confusing to you. If so, take the time to check it for yourself!</p>
<p>Second, we can also view matrix-matrix multiplication as a set of matrix-vector products. Specifically, if we represent <span class="math notranslate nohighlight">\(B\)</span> by columns, we can view the columns of <span class="math notranslate nohighlight">\(C\)</span> as matrix-vector products between <span class="math notranslate nohighlight">\(A\)</span> and the columns of <span class="math notranslate nohighlight">\(B\)</span>. Symbolically,</p>
<br>
<div class="math notranslate nohighlight">
\[\begin{split}
C = AB = A \begin{bmatrix}
      | &amp; | &amp; | &amp; ... &amp; | \\
      b^1 &amp; b^2 &amp; b^3 &amp; ... &amp; b^n \\
      | &amp; | &amp; | &amp; ... &amp;|
      \end{bmatrix} = \begin{bmatrix}
      | &amp; | &amp; | &amp; ... &amp; | \\
      Ab^1 &amp; Ab^2 &amp; Ab^3 &amp; ... &amp; Ab^n \\
      | &amp; | &amp; | &amp; ... &amp;|
      \end{bmatrix} \color{blue}{\enspace \rightarrow \enspace (2)}
\end{split}\]</div>
<br>
</div>
</div>
<div class="section" id="operations-and-properties">
<h2>2. Operations and Properties<a class="headerlink" href="#operations-and-properties" title="Permalink to this headline">¶</a></h2>
<div class="section" id="symmetric-matrices">
<h3>2.1. Symmetric Matrices<a class="headerlink" href="#symmetric-matrices" title="Permalink to this headline">¶</a></h3>
<p>A square matrix <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n \times n}\)</span> is <strong>symmetric</strong> if <span class="math notranslate nohighlight">\(A=A^T\)</span>. It is <strong>anti-symmetric</strong> if <span class="math notranslate nohighlight">\(A=-A^T\)</span>. It is easy to show that any matrix <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n \times n}\)</span>, the matrix <span class="math notranslate nohighlight">\(A+A^T\)</span> is symmetric and the matrix <span class="math notranslate nohighlight">\(A-A^T\)</span> is anti-symmetric. From this it follows that any square matrix <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n \times n}\)</span> can be be represented as a sum of a symmetric matrix and an anti-symmetric matrix as shown below:</p>
<br>
<div class="math notranslate nohighlight">
\[A = \frac{1}{2}(A+A^T)+\frac{1}{2}(A-A^T)\]</div>
<br>
</div>
<div class="section" id="norms">
<h3>2.2. Norms<a class="headerlink" href="#norms" title="Permalink to this headline">¶</a></h3>
<p>A norm, <span class="math notranslate nohighlight">\(||x||\)</span>, of a vector is informally defined as the measure of ‘length’ of the vector. For example, we have the commonly used Eucledian or <span class="math notranslate nohighlight">\(l_{2}\)</span> norm,</p>
<br>
<div class="math notranslate nohighlight">
\[||x||_{2} = \sqrt{\sum_{i=1}^{n}x_{i}^2}\]</div>
<br>
<p>Note that <span class="math notranslate nohighlight">\(||x||_{2}^2 = x^Tx\)</span></p>
<p>More formally, norm is a function <span class="math notranslate nohighlight">\(f : \mathbb{R}^n \rightarrow \mathbb{R}\)</span> that satisfies <span class="math notranslate nohighlight">\(4\)</span> properties:</p>
<p style="line-height:180%;">
<p><span class="math notranslate nohighlight">\(\rightarrow \text{For all } x \in \mathbb{R}^{n}, f(x) \geq 0. \text{ (non-negativity)}\)</span></p>
<p><span class="math notranslate nohighlight">\(\rightarrow f(x)=0 \text{ if and only if } x=0 \text{ (definiteness)}\)</span></p>
<p><span class="math notranslate nohighlight">\(\rightarrow \text{For all } x \in \mathbb{R}^{n}, t \in \mathbb{R}, f(tx) = |t|f(x). \text{ (homogeneity)}\)</span></p>
<p><span class="math notranslate nohighlight">\(\rightarrow \text{For all } x,y \in \mathbb{R}^{n}, f(x+y) \leq f(x) +f(y). \text{ (traingle inequality)}\)</span></p>
</p>
<p>Other examples of norms are the <span class="math notranslate nohighlight">\(l_1\)</span> norm,</p>
<div class="math notranslate nohighlight">
\[||x||_1 = \sum_{i=1}^{n}|x_i|\]</div>
<p>and the <span class="math notranslate nohighlight">\(l_\infty\)</span> norm,</p>
<div class="math notranslate nohighlight">
\[||x||_\infty = max_i|x_i|\]</div>
<p>In fact, all three norms presented so far are examples of the family of <span class="math notranslate nohighlight">\(l_p\)</span> norms, which are parameterized by a real number <span class="math notranslate nohighlight">\(p \geq 1\)</span> and defined as</p>
<div class="math notranslate nohighlight">
\[||x||_p = \left( \sum_{i=1}^n|x_i|^p\right)^{1/p}\]</div>
<br><p>Norms can also be defined for matrices, such as the Frobenius norm,</p>
<div class="math notranslate nohighlight">
\[\begin{align}||A||_{F} = \sqrt{\sum_{i=1}^{m}\sum_{j=1}^{n}A_{ij}^2} = \sqrt{trace(A^TA)}\end{align}\]</div>
<br>
</div>
<div class="section" id="linear-independence-and-rank">
<h3>2.3. Linear Independence and Rank<a class="headerlink" href="#linear-independence-and-rank" title="Permalink to this headline">¶</a></h3>
<p>A set of vectors <span class="math notranslate nohighlight">\(\{x_1, x_2,..,x_n\} \subset \mathbb{R}^n\)</span> is set to be <strong>linearly independent</strong> if no vector can be represented as a linear combination of the remaining vectors. Conversely, if one vector belonging to the set can be represented as a linear combination of the remaining vectors, then the vectors are said to be <strong>linearly dependent</strong>. That is, if</p>
<div class="math notranslate nohighlight">
\[x_n = \sum_{i=1}^{n-1}\alpha_i x_i\]</div>
<br>
<p>for some scalar values <span class="math notranslate nohighlight">\(\alpha_1,...,\alpha_{n-1} \in \mathbb{R}\)</span>,then we say that the vectors <span class="math notranslate nohighlight">\(\{x_1, x_2,..,x_n\}\)</span> are linearly dependent; otherwise, the vectors are linearly independent. For example, the vectors</p>
<br>
<div class="math notranslate nohighlight">
\[\begin{split}x_1=\begin{bmatrix}
1 \\
2\\
3\end{bmatrix}\enspace x_2 = \begin{bmatrix}
4 \\
1\\ 
5 \end{bmatrix}\enspace x_3 = \begin{bmatrix}
2 \\
-3 \\
-1\end{bmatrix}\end{split}\]</div>
<br>
<p>are linearly dependent because <span class="math notranslate nohighlight">\(x_3 = -2x_1+x_2\)</span>.</p>
<p><strong><em>Column Rank</em></strong></p>
<p>The column rank of a matrix <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{m \times n}\)</span> is the size of the largest subset of columns of <span class="math notranslate nohighlight">\(A\)</span> that constitute a linearly independent set.</p>
<p><strong><em>Row Rank</em></strong></p>
<p>The row rank of a matrix <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{m \times n}\)</span> is the size of the largest subset of rows of <span class="math notranslate nohighlight">\(A\)</span> that constitute a linearly independent set.</p>
<p>If, for matrix column rank is equal to row rank, then both quantities are collectively referred to as the <strong>rank of the matrix <span class="math notranslate nohighlight">\(A\)</span></strong></p>
<p style="line-height:180%;">
<p><span class="math notranslate nohighlight">\(\rightarrow \text{For } A \in \mathbb{R}^{m \times n}, \text{ rank}(A) \leq min(m,n). \text{ If rank}(A) = min(m,n), \text{then } A \text{ is said to be } \textbf{full rank}\)</span></p>
<p><span class="math notranslate nohighlight">\(\rightarrow \text{For } A \in \mathbb{R}^{m \times n}, \text{ rank}(A) = \text{ rank}(A^T)\)</span></p>
<p><span class="math notranslate nohighlight">\(\rightarrow \text{For } A,B \in \mathbb{R}^{m \times n},\text{ rank}(A+B) \leq \text{ rank}(A) + \text{ rank}(B)\)</span></p>
</p>
<br>
</div>
<div class="section" id="the-inverse-of-a-square-matrix">
<h3>2.4. The Inverse of a Square Matrix<a class="headerlink" href="#the-inverse-of-a-square-matrix" title="Permalink to this headline">¶</a></h3>
<p>The <strong>inverse</strong> of a square matrix <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n \times n}\)</span> is denoted <span class="math notranslate nohighlight">\(A^{-1}\)</span>, and is the unique matrix such that:</p>
<div class="math notranslate nohighlight">
\[A^{-1}A=I=AA^{-1}\]</div>
<p>Note that not all matrices have inverses. Non-square matrices, for example, do not have inverse by definition. However, for some square matrices <span class="math notranslate nohighlight">\(A\)</span>, it may stil be the case that <span class="math notranslate nohighlight">\(A^{-1}\)</span> may not exist. In particular, we say that <span class="math notranslate nohighlight">\(A\)</span> is <strong>invertible</strong> or <strong>non-singular</strong> if <span class="math notranslate nohighlight">\(A^{-1}\)</span> exists and <strong>non-invertible</strong> or <strong>singular</strong> otherwise.</p>
<p>In order for a square matrix <span class="math notranslate nohighlight">\(A\)</span> to have an inverse <span class="math notranslate nohighlight">\(A^{-1}\)</span>, <span class="math notranslate nohighlight">\(A\)</span> must be of full rank.</p>
<p>The following are properties of the inverse; all assume that <span class="math notranslate nohighlight">\(A, B \in \mathbb{R}^{n \times n}\)</span> are non-singular:</p>
<p style="line-height:180%;">
<p><span class="math notranslate nohighlight">\(\rightarrow (A^{-1})^{-1}=A\)</span></p>
<p><span class="math notranslate nohighlight">\(\rightarrow (AB)^{-1} = B^{-1}A^{-1}\)</span></p>
<p><span class="math notranslate nohighlight">\(\rightarrow (A^{-1})^T = (A^T)^{-1}\)</span></p>
<p><span class="math notranslate nohighlight">\((A^T)^{-1}\)</span> is denoted by <span class="math notranslate nohighlight">\(A^{-T}\)</span></p>
</p>
<br>
</div>
<div class="section" id="orthogonal-matrices">
<h3>2.5. Orthogonal Matrices<a class="headerlink" href="#orthogonal-matrices" title="Permalink to this headline">¶</a></h3>
<p>Two vectors <span class="math notranslate nohighlight">\(x,y \in \mathbb{R}^n\)</span> are <strong>orthogonal</strong> if <span class="math notranslate nohighlight">\(x^Ty=0\)</span>. A vector <span class="math notranslate nohighlight">\(x \in \mathbb{R}^n\)</span> is <strong>normalized</strong> if <span class="math notranslate nohighlight">\(||x||_2=1\)</span>. A square matrix <span class="math notranslate nohighlight">\(U \in \mathbb{R}^{n \times n}\)</span> is <strong>orthogonal</strong> <em>(note the different meanings when talking about vectors versus matrices)</em> if all its columns are orthogonal to each other and are normalized.</p>
<p>It follows immediately from the definition of orthogonality and normality that</p>
<br>
<div class="math notranslate nohighlight">
\[U^TU = I = UU^T\]</div>
<br>
<p>In other words, the inverse of an orthogonal matrix is its transpose. Note that if <span class="math notranslate nohighlight">\(U\)</span> is not square (i.e., <span class="math notranslate nohighlight">\(U \in \mathbb{R}^{m \times n}, \enspace n &lt; m\)</span>) but its columns are still orthonormal, then <span class="math notranslate nohighlight">\(U^TU=I\)</span>, but <span class="math notranslate nohighlight">\(UU^T \neq I\)</span>.</p>
<p>Another nice property of orthogonal matrices is that operating on a vector with an orthogonal matrix will not change its <em>Euclidean norm</em>. i.e.,</p>
<div class="math notranslate nohighlight">
\[||Ux||_2 = ||x||_2 \color{blue}{\enspace \rightarrow \enspace (3)}\]</div>
<br>
<p>for any <span class="math notranslate nohighlight">\(x \in \mathbb{R}^n, U \in \mathbb{R}^{n \times n}\)</span> orthogonal.</p>
</div>
<div class="section" id="range-and-nullspace-of-a-matrix">
<h3>2.6. Range and nullspace of a Matrix<a class="headerlink" href="#range-and-nullspace-of-a-matrix" title="Permalink to this headline">¶</a></h3>
<p>The span of a set of vectors <span class="math notranslate nohighlight">\(\begin{Bmatrix} x_{1},x_{2},...x_{n} \end{Bmatrix}\)</span> is the set of all vectors that can be expressed as a linear combination of <span class="math notranslate nohighlight">\(\begin{Bmatrix} x_{1},x_{2},...x_{n} \end{Bmatrix}\)</span>. That is,</p>
<br>
<div class="math notranslate nohighlight">
\[\text{span}(\begin{Bmatrix} x_{1},x_{2},...x_{n} \end{Bmatrix}) = \left \{ v:v = \sum_{i=1}^{n} \alpha_ix_i, \enspace \alpha_i \in \mathbb{R} \right \}\]</div>
<br>
<p>It can be shown that if <span class="math notranslate nohighlight">\(\begin{Bmatrix} x_{1},x_{2},...x_{n} \end{Bmatrix}\)</span> is a set of <span class="math notranslate nohighlight">\(n\)</span> linearly independent vectors, where each <span class="math notranslate nohighlight">\(x_i \in \mathbb{R}^n\)</span>, then <span class="math notranslate nohighlight">\(\text{span}(\begin{Bmatrix} x_{1},x_{2},...x_{n} \end{Bmatrix}) = \mathbb{R}^n\)</span>. In other words, any vector <span class="math notranslate nohighlight">\(v \in \mathbb{R}^n\)</span> can be written as a linear combination of <span class="math notranslate nohighlight">\(x_1\)</span> through <span class="math notranslate nohighlight">\(x_n\)</span>.</p>
<p>The <strong>projection</strong> of a vector <span class="math notranslate nohighlight">\(y \in \mathbb{R}^m\)</span> onto the span of <span class="math notranslate nohighlight">\(\begin{Bmatrix} x_{1},x_{2},...x_{n} \end{Bmatrix}\)</span> (here we assume <span class="math notranslate nohighlight">\(x_i \in \mathbb{R}^m\)</span>) is a vector <span class="math notranslate nohighlight">\(v \in \text{span}(\begin{Bmatrix} x_{1},x_{2},...x_{n} \end{Bmatrix})\)</span>, such that <span class="math notranslate nohighlight">\(v\)</span> is as close as possible to <span class="math notranslate nohighlight">\(y\)</span>, as measured by the Euclidean norm <span class="math notranslate nohighlight">\(||v-y||_2\)</span>. We denote the projection as <span class="math notranslate nohighlight">\(\text{Proj}(y;\begin{Bmatrix} x_{1},x_{2},...x_{n} \end{Bmatrix})\)</span> and can define it formally as,</p>
<br>
<div class="math notranslate nohighlight">
\[\text{Proj}(y;\begin{Bmatrix}x_{1},...,x_{n}\end{Bmatrix}) = \text{argmin}_{v \in span(\begin{Bmatrix}x_{1},...,x_{n}\end{Bmatrix})}||y-v||_{2}\]</div>
<br>
<p>The range of a matrix <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{m \times n}\)</span>, denoted by <span class="math notranslate nohighlight">\(R(A)\)</span>, is the span of the columns of <span class="math notranslate nohighlight">\(A\)</span>. In other words,</p>
<br>
<div class="math notranslate nohighlight">
\[R(A) = \begin{Bmatrix}v \in \mathbb{R}^m : v = Ax, x \in \mathbb{R}^n \end{Bmatrix}\]</div>
<br>
<p>The nullspace of a matrix <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{m \times n}\)</span>, denoted by <span class="math notranslate nohighlight">\(\mathcal{N}(A)\)</span> is the set of all vectors that equal to <span class="math notranslate nohighlight">\(0\)</span> when multiplied by <span class="math notranslate nohighlight">\(A\)</span>, i.e.,</p>
<div class="math notranslate nohighlight">
\[\mathcal{N}(A) = \begin{Bmatrix} x \in \mathbb{R}^n : Ax = 0\end{Bmatrix}\]</div>
<br>
</div>
<div class="section" id="quadratic-forms-and-positive-semidefinite-matrices">
<h3>2.7. Quadratic Forms and Positive Semidefinite Matrices<a class="headerlink" href="#quadratic-forms-and-positive-semidefinite-matrices" title="Permalink to this headline">¶</a></h3>
<p>Given a square matrix <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n \times n}\)</span> and a vector <span class="math notranslate nohighlight">\(x \in \mathbb{R}^{n}\)</span>, the scalar value <span class="math notranslate nohighlight">\(x^TAx\)</span> is called a <em>quadratic form</em>. Written explicitly, we see that</p>
<br>
<div class="math notranslate nohighlight">
\[\begin{align}x^TAx = \sum_{i=1}^{n}x_{i}(Ax)_{i} = \sum_{i=1}^{n}x_{i}\left (\sum_{j=1}^{n}A_{ij}x_{j}\right ) = \sum_{i=1}^{n}\sum_{j=1}^{n}A_{ij}x_{i}x_{j}\end{align}\]</div>
<br>
<p>Note that,</p>
<div class="math notranslate nohighlight">
\[x^TAx = (x^TAx)^T = x^TA^Tx = x^T \left( \frac{1}{2}A + \frac{1}{2}A^T\right)x\]</div>
<br>
<p>where the first equality follows from the fact that the transpose of a scalar is equal to itself, and the second equality follows from the fact that we are averaging two quantities which are themselves equal. From this, we can conclude that only the symmetric part of <span class="math notranslate nohighlight">\(A\)</span> contributes to the quadratic form. For this reason, we often implicitly assume that the matrices appearing in the quadratic form are symmetric.</p>
<p>We have the following definitions:</p>
<ul class="simple">
<li><p>A symmetric matrix <span class="math notranslate nohighlight">\(A \in \mathbb{S}^n\)</span> is <span class = 'high'>positive definite</span> (PD) if for all non-zero vectors <span class="math notranslate nohighlight">\(x \in \mathbb{R}^n, x^TAx&gt;0\)</span>. This is usually denoted <span class="math notranslate nohighlight">\(A\succ0\)</span>, and often times the set of all positive definite matrices is denoted by <span class="math notranslate nohighlight">\(\mathbb{S}_{++}^n\)</span></p></li>
</ul>
<br>
<ul class="simple">
<li><p>A symmetric matrix <span class="math notranslate nohighlight">\(A \in \mathbb{S}^n\)</span> is <span class = 'high'>positive semidefinite</span> (PSD) if for all non-zero vectors <span class="math notranslate nohighlight">\(x \in \mathbb{R}^n, x^TAx \geq 0\)</span>. This is usually denoted <span class="math notranslate nohighlight">\(A \succeq 0\)</span>, and often times the set of all positive semidefinite matrices is denoted by <span class="math notranslate nohighlight">\(\mathbb{S}_{+}^n\)</span></p></li>
</ul>
<br>
<ul class="simple">
<li><p>A symmetric matrix <span class="math notranslate nohighlight">\(A \in \mathbb{S}^n\)</span> is <span class = 'high'>negative definite</span> (ND), denoted <span class="math notranslate nohighlight">\(A\prec0\)</span> if for all non-zero vectors <span class="math notranslate nohighlight">\(x \in \mathbb{R}^n, x^TAx&lt;0\)</span>.</p></li>
</ul>
<br>
<ul class="simple">
<li><p>A symmetric matrix <span class="math notranslate nohighlight">\(A \in \mathbb{S}^n\)</span> is <span class = 'high'>negative semidefinite</span> (NSD), denoted <span class="math notranslate nohighlight">\(A \preceq 0\)</span> if for all non-zero vectors <span class="math notranslate nohighlight">\(x \in \mathbb{R}^n, x^TAx \leq 0\)</span>.</p></li>
</ul>
<br>
<ul class="simple">
<li><p>A symmetric matrix <span class="math notranslate nohighlight">\(A \in \mathbb{S}^n\)</span> is <span class = 'high'>indefinite</span>, if it is neither positive semidefinite nor negative semidefinite - i.e., if there exists <span class="math notranslate nohighlight">\(x_{1}, x_{2} \in \mathbb{R}^n \)</span> such that <span class="math notranslate nohighlight">\(x_{1}^TAx_{1}&gt;0\)</span> and <span class="math notranslate nohighlight">\(x_2^TAx_2&lt;0\)</span>.</p></li>
</ul>
<p>It should be obvious that if <span class="math notranslate nohighlight">\(A\)</span> is positive definite, then <span class="math notranslate nohighlight">\(-A\)</span> is negative definite and vice versa. Likewise, if <span class="math notranslate nohighlight">\(A\)</span> is positive semidefinite then <span class="math notranslate nohighlight">\(-A\)</span> is negative semidefinite and vice versa. If <span class="math notranslate nohighlight">\(A\)</span> is indefinite, then so is <span class="math notranslate nohighlight">\(-A\)</span>.</p>
<p>One important property of positive definite and negative definite matrices is that they are always full rank, and hence, invertible. To see why this is the case, suppose that some matrix <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n \times n}\)</span> is not full rank. Then, suppose that the <span class="math notranslate nohighlight">\(j\)</span>th column of <span class="math notranslate nohighlight">\(A\)</span> is expressible as a linear combination of other <span class="math notranslate nohighlight">\(n-1\)</span> columns:</p>
<div class="math notranslate nohighlight">
\[a_j = \sum_{i \neq j} x_ia_i\]</div>
<p>for some <span class="math notranslate nohighlight">\(x_1,..x_{j-1},x_{j+1},...,x_n \in \mathbb{R}\)</span>. Setting <span class="math notranslate nohighlight">\(x_j=-1\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[Ax = \sum_{i=1}^n x_ia_i=0\]</div>
<p>But this implies <span class="math notranslate nohighlight">\(x^TAx=0\)</span> for some non-zero vector <span class="math notranslate nohighlight">\(x\)</span>, so <span class="math notranslate nohighlight">\(A\)</span> must be neither positive definite nor negative definite. Therefore, if <span class="math notranslate nohighlight">\(A\)</span> is either positive definite or negative definite, it must be full rank.</p>
<p>Finally, there is one type of positive definite matrix that comes up frequently, and so deserves some special mention. Given any matrix <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{m \times n}\)</span> (not necessarily symmetric or even square), the matrix <span class="math notranslate nohighlight">\(G = A^TA\)</span> (sometimes called a <strong>Gram Matrix</strong>) is always positive semidefinite. Further, if <span class="math notranslate nohighlight">\(m \geq n\)</span> (and we assume for convenience that A is full rank), then <span class="math notranslate nohighlight">\(G=A^TA\)</span> is positive definite.</p>
<br>
</div>
<div class="section" id="eigenvalues-and-eigenvectors">
<h3>2.8. Eigenvalues and Eigenvectors<a class="headerlink" href="#eigenvalues-and-eigenvectors" title="Permalink to this headline">¶</a></h3>
<p>Given a square matrix <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n \times n}\)</span>, we say that <span class="math notranslate nohighlight">\(\lambda \in \mathbb{C}\)</span> is an <strong>eigenvalue</strong> if <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(x \in \mathbb{C}^n\)</span> is the corresponding <strong>eigenvector</strong> if</p>
<div class="math notranslate nohighlight">
\[ Ax = \lambda x, \enspace x \neq 0 \]</div>
<p>Intuitively, this definition means that multiplying <span class="math notranslate nohighlight">\(A\)</span> by the vector <span class="math notranslate nohighlight">\(x\)</span> results in a new vector that points in the same direction as <span class="math notranslate nohighlight">\(x\)</span>, but scaled by a factor <span class="math notranslate nohighlight">\(\lambda\)</span>. Also note that for any eigenvector <span class="math notranslate nohighlight">\(x \in \mathbb{C}^n\)</span> and scalar <span class="math notranslate nohighlight">\(t \in \mathbb{C}, A(cx) = cAx = c \lambda x = \lambda (cx)\)</span>, so <span class="math notranslate nohighlight">\(cx\)</span> is also an eigenvector. For this reason when we talk about ‘the’ eigenvector associated with <span class="math notranslate nohighlight">\(\lambda\)</span>, we usually assume that the eigenvector is normalized to have length <span class="math notranslate nohighlight">\(1\)</span>.</p>
<p>We can rewrite the equation above to state that <span class="math notranslate nohighlight">\((\lambda, x)\)</span> is an eigenvalue-eigenvector pair of <span class="math notranslate nohighlight">\(A\)</span> if,</p>
<div class="math notranslate nohighlight">
\[(\lambda I -A)x =0, \enspace x \neq 0\]</div>
<br>
<p>But <span class="math notranslate nohighlight">\((\lambda I -A)x =0\)</span> has a non-zero solution to <span class="math notranslate nohighlight">\(x\)</span> if and only if <span class="math notranslate nohighlight">\((\lambda I -A)\)</span> has a non-empty nullspace, which is only the case if <span class="math notranslate nohighlight">\((\lambda I -A)\)</span> is singular, i.e.,</p>
<div class="math notranslate nohighlight">
\[|(\lambda I -A)|=0\]</div>
<br><p>We can now use the definition of the determinant to expand this expression <span class="math notranslate nohighlight">\(|(\lambda I -A)|\)</span> into a polynomial in <span class="math notranslate nohighlight">\(\lambda\)</span>, where <span class="math notranslate nohighlight">\(\lambda\)</span> will have degree <span class="math notranslate nohighlight">\(n\)</span>. It’s often called the characteristic polynomial of the matrix <span class="math notranslate nohighlight">\(A\)</span>.</p>
<p>We then find the <span class="math notranslate nohighlight">\(n\)</span> roots of this characteristic polynomial and denote them by <span class="math notranslate nohighlight">\(\lambda_1, \lambda_2,...,\lambda_n\)</span>. These are all the eigenvalues of the matrix <span class="math notranslate nohighlight">\(A\)</span>.</p>
<p>The following are the properties of eigenvalues and eigenvectors:</p>
<ul class="simple">
<li><p>The trace of <span class="math notranslate nohighlight">\(A\)</span> is equal to the sum of its eigenvalues,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[trace(A) = \sum_{i=1}^n \lambda_i\]</div>
<ul class="simple">
<li><p>The determinant of <span class="math notranslate nohighlight">\(A\)</span> is equal to the product of its eigenvalues,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[|A| = \prod_{i=1}^n \lambda_i\]</div>
<ul class="simple">
<li><p>The rank of <span class="math notranslate nohighlight">\(A\)</span> is equal to the number of non-zero eigenvalues of <span class="math notranslate nohighlight">\(A\)</span>.</p></li>
</ul>
<br>
<ul class="simple">
<li><p>Supppose <span class="math notranslate nohighlight">\(A\)</span> is non-singular with eigenvalue <span class="math notranslate nohighlight">\(\lambda\)</span> and an associated eigenvector <span class="math notranslate nohighlight">\(x\)</span>. Then <span class="math notranslate nohighlight">\(\frac{1}{\lambda}\)</span> is an eigenvalue of <span class="math notranslate nohighlight">\(A^{-1}\)</span> with an associated eigenvector <span class="math notranslate nohighlight">\(x\)</span>, i.e., <span class="math notranslate nohighlight">\(A^{-1}x = (\frac{1}{\lambda})x\)</span>.</p></li>
</ul>
<br>
<ul class="simple">
<li><p>The eigenvalues of a diagonal matrix <span class="math notranslate nohighlight">\(D = \text{diag}(d_1,...,d_n)\)</span> are just the diagonal entries <span class="math notranslate nohighlight">\(d_1,...,d_n\)</span></p></li>
</ul>
<br>
</div>
<div class="section" id="eigenvalues-and-eigenvectors-of-symmetric-matrices">
<h3>2.9. Eigenvalues and Eigenvectors of Symmetric Matrices<a class="headerlink" href="#eigenvalues-and-eigenvectors-of-symmetric-matrices" title="Permalink to this headline">¶</a></h3>
<p>In general, the structures of the eigenvalues and eigenvectors of a general square matrix can be subtle to characterize. Fortunately, in most of the cases in machine learning, if suffices to deal with symmetric real matrices, whose eigenvalues and eigenvectors have remarkable properties.</p>
<p>Throughout this section, let’s assume that <span class="math notranslate nohighlight">\(A\)</span> is a symmetric real matrix. We have the following properties:</p>
<ul class="simple">
<li><p>All eigenvalues of <span class="math notranslate nohighlight">\(A\)</span> are real numbers. We denote them by <span class="math notranslate nohighlight">\(\lambda_1,...,\lambda_n\)</span></p></li>
</ul>
<br>
<ul class="simple">
<li><p>There exists a set of eigenvectors <span class="math notranslate nohighlight">\(u_1,...,u_n\)</span> such that:</p></li>
</ul>
<p>a) For all <span class="math notranslate nohighlight">\(i, u_i\)</span> is an eigenvector with eigenvalue <span class="math notranslate nohighlight">\(\lambda_i\)</span> and</p>
<p>b) <span class="math notranslate nohighlight">\(u_1,...,u_n\)</span> are unit vectors and orthogonal to each other.</p>
<p>Let <span class="math notranslate nohighlight">\(U\)</span> be the orthonormal matrix that contains <span class="math notranslate nohighlight">\(u_i\)</span>’s as columns:</p>
<br>
<div class="math notranslate nohighlight">
\[\begin{split}U = \begin{bmatrix}
      | &amp; | &amp; | &amp; ... &amp; | \\
      u^1 &amp; u^2 &amp; u^3 &amp; ... &amp; u^n \\
      | &amp; | &amp; | &amp; ... &amp;|
      \end{bmatrix} \color{blue}{\enspace \rightarrow \enspace (4)}\end{split}\]</div>
<br>
<p>Let <span class="math notranslate nohighlight">\(\Lambda = \text{diag}(\lambda_1,...,\lambda_n)\)</span> be the diagonal matrix that contains <span class="math notranslate nohighlight">\(\lambda_1,...,\lambda_n\)</span> as entries on the diagonal. Using the view of matrix-matrix vector multiplication in <a href="#matrix-matrix-products" ><span class="math notranslate nohighlight">\(\small{\color{blue}{\text{equation }(2)}}\)</span></a>, we can verify that</p>
<br>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{align}AU = \begin{bmatrix}
      | &amp; | &amp; | &amp; ... &amp; | \\
      Au^1 &amp; Au^2 &amp; Au^3 &amp; ... &amp; Au^n \\
      | &amp; | &amp; | &amp; ... &amp;|
      \end{bmatrix} &amp;= \begin{bmatrix}
      | &amp; | &amp; | &amp; ... &amp; | \\
      \lambda_1u^1 &amp; \lambda_2u^2 &amp; \lambda_3u^3 &amp; ... &amp; \lambda_nu^n \\
      | &amp; | &amp; | &amp; ... &amp;|
      \end{bmatrix} \\ \\ &amp;= U\text{diag}(\lambda_1,...,\lambda_n) \\ \\&amp;= U\Lambda \end{align}\end{split}\]</div>
<br>
<p>Recalling that orthonormal matrix <span class="math notranslate nohighlight">\(U\)</span> satisfies <span class="math notranslate nohighlight">\(UU^T=I\)</span> and using the equation above, we have</p>
<br>
<div class="math notranslate nohighlight">
\[A = AUU^T = U \Lambda U^T \color{blue}{\enspace \rightarrow \enspace (5)}\]</div>
<br>
<p>This new presentation of <span class="math notranslate nohighlight">\(A\)</span> as  <span class="math notranslate nohighlight">\(U \Lambda U^T\)</span> is often called <span class = 'high'>diagonalization</span> of the matrix <span class="math notranslate nohighlight">\(A\)</span>. The term diagonalization comes from the fact that with such representation, we can often effectively treat a symmetric matrix <span class="math notranslate nohighlight">\(A\)</span> as a diagonal matrix - which is much easier to understand - w.r.t. the basis defined by the eigenvectors <span class="math notranslate nohighlight">\(U\)</span>. We will elaborate this below by several examples.</p>
<p><em>Background: representing vector w.r.t. another basis:</em></p>
<br>
<p>Any orthonormal matrix <span class="math notranslate nohighlight">\(U = \begin{bmatrix}
      | &amp; | &amp; | &amp; ... &amp; | \\
      u^1 &amp; u^2 &amp; u^3 &amp; ... &amp; u^n \\
      | &amp; | &amp; | &amp; ... &amp;|
      \end{bmatrix}\)</span> defines a new basis (coordinate system) of <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span> in the following sense. For any vector <span class="math notranslate nohighlight">\(x \in \mathbb{R}^n\)</span> can be represented as a linear combination of <span class="math notranslate nohighlight">\(u_1,...,u_n\)</span> with coefficient <span class="math notranslate nohighlight">\(\hat{x_1},...,\hat{x_n}\)</span></p>
<div class="math notranslate nohighlight">
\[x = \hat{x_1}u_1+...+\hat{x_n}u_n = U\hat x\]</div>
<p>where in the second equality we use the view of <a href="#matrix-vector-products" ><span class="math notranslate nohighlight">\(\small{\color{blue}{\text{equation }(1)}}\)</span></a>. Indeed, such <span class="math notranslate nohighlight">\(\hat{x}\)</span> uniquely exists</p>
<div class="math notranslate nohighlight">
\[x = U\hat{x} = U^Tx = \hat{x}\]</div>
<p>In other words, the vector <span class="math notranslate nohighlight">\(\hat x = U^Tx\)</span> can serve as another representation of the vector <span class="math notranslate nohighlight">\(x\)</span> w.r.t. the basis defined by <span class="math notranslate nohighlight">\(U\)</span>.</p>
<br>
</div>
<div class="section" id="diagonalizing-matrix-vector-multiplication">
<h3>2.10. Diagonalizing matrix-vector multiplication:<a class="headerlink" href="#diagonalizing-matrix-vector-multiplication" title="Permalink to this headline">¶</a></h3>
<p>With the setup above, we will see that left-multiplying matrix <span class="math notranslate nohighlight">\(A\)</span> can be viewed as left-multiplying a diagonal matrix w.r.t. the basis of eigenvectors. Suppose <span class="math notranslate nohighlight">\(x\)</span> is a vector and <span class="math notranslate nohighlight">\(\hat x\)</span> is its representation w.r.t. to the basis of <span class="math notranslate nohighlight">\(U\)</span>. Let <span class="math notranslate nohighlight">\(z = Ax\)</span> be the matrix-vector product. Now, let’s compute the representation <span class="math notranslate nohighlight">\(z\)</span> w.r.t. the basis of <span class="math notranslate nohighlight">\(U\)</span>:</p>
<p>Then, again using the fact that <span class="math notranslate nohighlight">\(UU^T=U^TU=I\)</span> and <a href="#eigenvalues-and-eigenvectors-of-symmetric-matrices" ><span class="math notranslate nohighlight">\(\small{\color{blue}{\text{equation }(5)}}\)</span></a>, we have that</p>
<br>
<div class="math notranslate nohighlight">
\[\begin{split}\hat z = U^Tz = U^TAx = U^TU\Lambda U^Tx = \Lambda\hat x = \begin{bmatrix}
\lambda_1 \hat{x_1} \\
\lambda_2 \hat{x_2}\\
... \\
\lambda_n \hat{x_n}\end{bmatrix}\end{split}\]</div>
<br>
<p>We see that left-multiplying matrix <span class="math notranslate nohighlight">\(A\)</span> in the original space is equivalent to left-multiplying the diagonal matrix <span class="math notranslate nohighlight">\(\Lambda\)</span> w.r.t the new basis, which is merely scaling each coordinate by the value of corresponding eigenvalue.</p>
<p>Under the new basis, multiplying a matrix multiple times becomes much simpler as well. For example, suppose <span class="math notranslate nohighlight">\(q = AAAx\)</span>. Deriving out the analytical form of <span class="math notranslate nohighlight">\(q\)</span> in terms of the entries of <span class="math notranslate nohighlight">\(A\)</span> may be a nightmare under the original basis, but can be much easier under the new one:</p>
<br>
<div class="math notranslate nohighlight">
\[\begin{split}\hat{q} = U^Tq = U^TAx = U^TU\Lambda U^TU\Lambda U^TU\Lambda U^Tx = \Lambda^3\hat x = \begin{bmatrix}
\lambda_1^3 \hat{x_1} \\
\lambda_2^3 \hat{x_2}\\
... \\
\lambda_n^3 \hat{x_n}\end{bmatrix} \color{blue}{\enspace \rightarrow \enspace (6)}\end{split}\]</div>
<br>
<p><strong>“Diagonalizing” quadratic form:</strong></p>
<p>As a directly corollary, the quadratic form <span class="math notranslate nohighlight">\(x^TAx\)</span> can also be simplified under the new basis.</p>
<br>
<div class="math notranslate nohighlight">
\[x^TAx = x^TU\Lambda U^Tx = \hat x \Lambda \hat x = \sum_{i=1}^{n}\lambda_i \hat{x_i}^2 \color{blue}{\enspace \rightarrow \enspace (7)}\]</div>
<br>
<p>(Recall that with the old representation, <span class="math notranslate nohighlight">\(x^TAx = \sum_{i=1, j=1}^n x_i x_j A_{ij}\)</span> involves a sum of <span class="math notranslate nohighlight">\(n^2\)</span> terms instead of <span class="math notranslate nohighlight">\(n\)</span> terms in the equation above.) With this viewpoint, we can also show that the definiteness of the matrix <span class="math notranslate nohighlight">\(A\)</span> depends entirely on the sign of its eigenvalues:</p>
<p style="line-height:180%;">
<p><span class="math notranslate nohighlight">\(\rightarrow \text{If all } \lambda_i &gt; 0 \text{ then the matrix } A \text{ is positive definite because:}\)</span></p>
<div class="math notranslate nohighlight">
\[x^TAx = \sum_{i=1}^n \lambda_i \hat{x_i}^2 &gt; 0 \text{ for any } \hat x \neq 0\]</div>
<p><span class="math notranslate nohighlight">\(\rightarrow \text{If all } \lambda_i \geq 0 \text{ then the matrix } A \text{ is positive semi-definite because:}\)</span></p>
<div class="math notranslate nohighlight">
\[X^TAx  = \sum_{i=1}^n\lambda_i \hat{x_i}^2 \geq 0 \text{ for all }\hat x\]</div>
<p><span class="math notranslate nohighlight">\(\rightarrow \text{Likewise, if all } \lambda_i &lt;0 \text{ or } \lambda_i \leq 0 \text{, then A is negative definite or negative semidefinite respectively.}\)</span></p>
<p><span class="math notranslate nohighlight">\(\rightarrow \text{Finally, if A has both positive and negative eigenvalues, say } \lambda_i &gt;0 \text{ and } \lambda_j &lt;0,\)</span></p>
<p><span class="math notranslate nohighlight">\(\text{then it is indefinite.}\)</span></p>
<p>This is because if we let <span class="math notranslate nohighlight">\(\hat x\)</span> satisfy <span class="math notranslate nohighlight">\(\hat x_i = 1\)</span> and <span class="math notranslate nohighlight">\(\hat x_k = 0, \enspace \forall k\neq i\)</span>, then <span class="math notranslate nohighlight">\(x^TAx = \sum_{i=1}^{n}\lambda_i \hat{x_i}^2&gt;0\)</span>. Similarly, we can let <span class="math notranslate nohighlight">\(\hat x\)</span> satisfy <span class="math notranslate nohighlight">\(\hat{x_j}=1\)</span> and <span class="math notranslate nohighlight">\(\hat{x_k}=0, \enspace \forall k \neq j\)</span>, then <span class="math notranslate nohighlight">\(x^TAx = \sum_{i=1}^{n} \lambda_i \hat{x_i}^2 &lt;0\)</span>.</p>
</p>
<p>An application where eigenvalues and eigenvectors come up frequently is in maximizing some function of a matrix. In particular, for a matrix <span class="math notranslate nohighlight">\(A \in \mathbb{S}^n\)</span>, consider the following maximization problem,</p>
<br>
<div class="math notranslate nohighlight">
\[\text{max}_{x \in \mathbb{R}^n} \enspace x^TAx = \sum_{i=1}^{n} \lambda_i \hat{x_i}^2, \enspace \text{subject to } ||x||_2 ^ 2=1 \color{blue}{\enspace \rightarrow \enspace (8)}\]</div>
<br>
<p>i.e., we want to find the vector which maximizes the quadratic form. Assuming the eigenvalues are ordered as <span class="math notranslate nohighlight">\(\lambda_1 \geq \lambda_2 \geq ... \geq \lambda_n\)</span>, the optimal values of this optimization problem is <span class="math notranslate nohighlight">\(\lambda_1\)</span> and any eigenvector <span class="math notranslate nohighlight">\(u_1\)</span> corresponding to <span class="math notranslate nohighlight">\(\lambda_1\)</span> is one of the maximizers.</p>
<p>We show this by using the diagonalization technique: Note that <span class="math notranslate nohighlight">\(||x||_2=||\hat x||_2\)</span> by <a href="#orthogonal-matrices" ><span class="math notranslate nohighlight">\(\small{\color{blue}{\text{equation }(3)}}\)</span></a>, and using <a href="#diagonalizing-matrix-vector-multiplication" ><span class="math notranslate nohighlight">\(\small{\color{blue}{\text{equation }(7)}}\)</span></a>, we can rewrite the optimization <a href="#diagonalizing-matrix-vector-multiplication" ><span class="math notranslate nohighlight">\(\small{\color{blue}{\text{equation }(8)}}\)</span></a> as:</p>
<br>
<div class="math notranslate nohighlight">
\[\text{max}_{x \in \mathbb{R}^n} \enspace \hat{x}^T\Lambda \hat{x} = \sum_{i=1}^n \lambda_i\hat{x_i}^2 \enspace \text{ subject to } ||\hat x||_2^2=1 \color{blue}{\enspace \rightarrow \enspace (9)}\]</div>
<br>
<p>Then, we have that the objective is upper bounded by <span class="math notranslate nohighlight">\(\lambda_1\)</span>:</p>
<br>
<div class="math notranslate nohighlight">
\[\hat{x}^T \Lambda \hat x = \sum_{i=1}^n \lambda_i \hat{x_i}^2 \leq \sum_{i=1}^n \lambda_1 \hat{x_i}^2 = \lambda_1 \color{blue}{\enspace \rightarrow \enspace (10)}\]</div>
<br>
<p>Moreover, setting <span class="math notranslate nohighlight">\(\hat{x}=\begin{bmatrix}
1 \\
0\\
..\\
0\end{bmatrix}\)</span> achieves the equality in the equation above, and this corresponds to setting <span class="math notranslate nohighlight">\(x=u_1\)</span></p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "EssentialAI/cs229",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./lectures"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="../index.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">The Essential AI</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="Lecture-1.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Lecture 1: Introduction to Machine Learning</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Naresh Kumar<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>